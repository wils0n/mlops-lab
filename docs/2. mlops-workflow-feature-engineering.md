# ğŸ§  Tutorial MLOps: Feature Engineering de ProducciÃ³n

## ğŸ“‹ Tabla de Responsabilidades

| Entregable                        | Responsable         | Status               |
| --------------------------------- | ------------------- | -------------------- |
| ğŸ““ ExploraciÃ³n de caracterÃ­sticas | CientÃ­fico de datos | âœ… Completo          |
| ğŸ”§ Pipeline de transformaciÃ³n     | Ingeniero de MLOps  | ğŸ¯ **Este tutorial** |
| ğŸ¤– Preprocessor serializado       | Ingeniero de MLOps  | ğŸ¯ **Este tutorial** |
| ğŸ“Š Features listas para ML        | Ingeniero de MLOps  | ğŸ¯ **Este tutorial** |
| ğŸ”„ Script reutilizable            | Ingeniero de MLOps  | ğŸ¯ **Este tutorial** |

---

# ğŸ“˜ MLOps Workflow: Feature Engineering desde Notebook a Script

### ğŸ“š **Â¿QuÃ© recibe el MLOps Engineer?**

El cientÃ­fico de datos entrega un notebook `02_feature_engineering.ipynb` con:

```python
# ğŸ““ CÃ³digo exploratorio del Data Scientist
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Carga y exploraciÃ³n
df = pd.read_csv("../data/processed/cleaned_house_data.csv")
print(f"Dataset shape: {df.shape}")
df.head()

# Feature Engineering exploratorio
df['house_age'] = 2025 - df['year_built']
df['price_per_sqft'] = df['price'] / df['sqft']
df['bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']

# AnÃ¡lisis de correlaciones
plt.figure(figsize=(12, 8))
correlation_matrix = df[['price', 'sqft', 'house_age', 'price_per_sqft', 'bed_bath_ratio']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Encoding categÃ³rico manual
df_encoded = pd.get_dummies(df, columns=['location', 'condition'])

# NormalizaciÃ³n bÃ¡sica
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
numeric_cols = ['sqft', 'bedrooms', 'bathrooms', 'house_age', 'price_per_sqft', 'bed_bath_ratio']
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])
```

### âš¡ **TransformaciÃ³n a Pipeline de ProducciÃ³n**

El MLOps Engineer convierte esto en `src/features/engineer.py`:

```python
# src/features/engineer.py
import pandas as pd
import numpy as np
from datetime import datetime
import logging
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('feature-engineering')

def create_features(df):
    """Create new features from existing data."""
    logger.info("Creating new features")

    # Make a copy to avoid modifying the original dataframe
    df_featured = df.copy()

    # Calculate house age
    current_year = datetime.now().year
    df_featured['house_age'] = current_year - df_featured['year_built']
    logger.info("Created 'house_age' feature")

    # Price per square foot
    df_featured['price_per_sqft'] = df_featured['price'] / df_featured['sqft']
    logger.info("Created 'price_per_sqft' feature")

    # Bedroom to bathroom ratio
    df_featured['bed_bath_ratio'] = df_featured['bedrooms'] / df_featured['bathrooms']
    # Handle division by zero
    df_featured['bed_bath_ratio'] = df_featured['bed_bath_ratio'].replace([np.inf, -np.inf], np.nan)
    df_featured['bed_bath_ratio'] = df_featured['bed_bath_ratio'].fillna(0)
    logger.info("Created 'bed_bath_ratio' feature")

    # Do NOT one-hot encode categorical variables here; let the preprocessor handle it
    return df_featured

def create_preprocessor():
    """Create a preprocessing pipeline."""
    logger.info("Creating preprocessor pipeline")

    # Define feature groups
    categorical_features = ['location', 'condition']
    numerical_features = ['sqft', 'bedrooms', 'bathrooms', 'house_age', 'price_per_sqft', 'bed_bath_ratio']

    # Preprocessing for numerical features
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean'))
    ])

    # Preprocessing for categorical features
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combine preprocessors in a column transformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    return preprocessor

def run_feature_engineering(input_file, output_file, preprocessor_file):
    """Full feature engineering pipeline."""
    # Load cleaned data
    logger.info(f"Loading data from {input_file}")
    df = pd.read_csv(input_file)

    # Create features
    df_featured = create_features(df)
    logger.info(f"Created featured dataset with shape: {df_featured.shape}")

    # Create and fit the preprocessor
    preprocessor = create_preprocessor()
    X = df_featured.drop(columns=['price'], errors='ignore')  # Features only
    y = df_featured['price'] if 'price' in df_featured.columns else None  # Target column (if available)
    X_transformed = preprocessor.fit_transform(X)
    logger.info("Fitted the preprocessor and transformed the features")

    # Save the preprocessor
    joblib.dump(preprocessor, preprocessor_file)
    logger.info(f"Saved preprocessor to {preprocessor_file}")

    # Save fully preprocessed data
    df_transformed = pd.DataFrame(X_transformed)
    if y is not None:
        df_transformed['price'] = y.values
    df_transformed.to_csv(output_file, index=False)
    logger.info(f"Saved fully preprocessed data to {output_file}")

    return df_transformed

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Feature engineering for housing data.')
    parser.add_argument('--input', required=True, help='Path to cleaned CSV file')
    parser.add_argument('--output', required=True, help='Path for output CSV file (engineered features)')
    parser.add_argument('--preprocessor', required=True, help='Path for saving the preprocessor')

    args = parser.parse_args()

    run_feature_engineering(args.input, args.output, args.preprocessor)
```

## ğŸ”‘ **Diferencias Clave: ExploraciÃ³n vs Script**

### ğŸ““ **CÃ³digo del Data Scientist (Notebook)**

```python
# âŒ CÃ³digo exploratorio - No apto para producciÃ³n
df['house_age'] = 2025 - df['year_built']  # AÃ±o hardcodeado
df['bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']  # Sin manejo de divisiÃ³n por 0
df_encoded = pd.get_dummies(df, columns=['location'])  # Sin handle_unknown
scaler.fit_transform(df[numeric_cols])  # Fit y transform juntos (data leakage)
```

### ğŸ”§ **CÃ³digo del MLOps Engineer (Script)**

```python
# âœ… CÃ³digo de producciÃ³n - Robusto y escalable
class HouseFeaturesEngineer(BaseEstimator, TransformerMixin):
    def __init__(self, current_year=None):
        self.current_year = current_year or datetime.now().year  # âœ… Configurable

    def transform(self, X):
        X_transformed['bed_bath_ratio'] = X['bedrooms'] / X['bathrooms'].replace(0, np.nan)  # âœ… Manejo de divisiÃ³n por 0

Pipeline([
    ('feature_engineer', HouseFeaturesEngineer()),  # âœ… SeparaciÃ³n clara
    ('preprocessor', ColumnTransformer([
        ('onehot', OneHotEncoder(handle_unknown='ignore'))  # âœ… Manejo de categorÃ­as nuevas
    ]))
])
```

## ğŸš€ **Ejecutar el Script de Feature Engineering**

### 1. **Crear estructura de directorios:**

```bash
mkdir -p data/processed models/trained logs
```

### 2. **Ejecutar pipeline completo:**

```bash
python src/features/engineer.py   --input data/processed/cleaned_house_data.csv   --output data/processed/featured_house_data.csv   --preprocessor models/trained/preprocessor.pkl
```

### 3. **Output esperado:**

```
2025-07-26 09:35:28,293 - feature-engineering - INFO - Loading data from data/processed/cleaned_house_data.csv
2025-07-26 09:35:28,295 - feature-engineering - INFO - Creating new features
2025-07-26 09:35:28,295 - feature-engineering - INFO - Created 'house_age' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created 'price_per_sqft' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created 'bed_bath_ratio' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created featured dataset with shape: (77, 10)
2025-07-26 09:35:28,296 - feature-engineering - INFO - Creating preprocessor pipeline
2025-07-26 09:35:28,303 - feature-engineering - INFO - Fitted the preprocessor and transformed the features
2025-07-26 09:35:28,304 - feature-engineering - INFO - Saved preprocessor to models/trained/preprocessor.pkl
2025-07-26 09:35:28,306 - feature-engineering - INFO - Saved fully preprocessed data to data/processed/featured_house_data.cs
```

## ğŸ“¦ **Archivos Generados**

### 1. **`featured_house_data.csv`**

```csv
# Datos transformados con todas las caracterÃ­sticas engineered
num__sqft,num__bedrooms,num__bathrooms,num__price_per_sqft,num__house_age,num__bed_bath_ratio,num__sqft_per_bedroom,num__is_new_house,num__is_large_house,cat__location_Downtown,cat__location_Mountain,cat__location_Rural,cat__location_Suburb,cat__location_Urban,cat__location_Waterfront,cat__condition_Excellent,cat__condition_Fair,cat__condition_Good,cat__condition_Poor,cat__location_category_Premium,cat__location_category_Rural,cat__location_category_Suburban,cat__location_category_Urban,price
-0.23,0.14,-0.45,1.2,0.67,0.89,-0.12,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,250000
```

### 2. **`preprocessor.pkl`**

- Pipeline completo de sklearn serializado
- Incluye HouseFeaturesEngineer + ColumnTransformer
- Contiene todas las transformaciones aprendidas (scaling, encoding, etc.)
- Listo para usar en producciÃ³n sin reentrenamiento

## ğŸ¯ **Â¿Por quÃ© este Script es Superior?**

### **ğŸ”„ ReutilizaciÃ³n Perfecta:**

```python
# Entrenamiento
pipeline = joblib.load('models/trained/preprocessor.pkl')
X_train_transformed = pipeline.fit_transform(X_train)
model.fit(X_train_transformed, y_train)

# ProducciÃ³n (mismas transformaciones automÃ¡ticamente)
X_new_transformed = pipeline.transform(X_new)
predictions = model.predict(X_new_transformed)
```

### **ğŸ›¡ï¸ Robustez de ProducciÃ³n:**

1. **Manejo de valores faltantes**: SimpleImputer con estrategias configurables
2. **CategorÃ­as nuevas**: OneHotEncoder con `handle_unknown='ignore'`
3. **DivisiÃ³n por cero**: Reemplazo seguro en ratios
4. **Escalabilidad**: Funciona con 1 fila o 1 millÃ³n de filas
5. **Versionado**: Pipelines serializados para control de versiones

### **ğŸ“Š CaracterÃ­sticas Creadas:**

| CaracterÃ­stica     | DescripciÃ³n               | Valor de Negocio      |
| ------------------ | ------------------------- | --------------------- |
| `house_age`        | AÃ±os desde construcciÃ³n   | Depreciation modeling |
| `price_per_sqft`   | Precio por pie cuadrado   | ComparaciÃ³n de valor  |
| `sqft_per_bedroom` | Superficie por habitaciÃ³n | Space optimization    |
| `bed_bath_ratio`   | Ratio habitaciones/baÃ±os  | Layout efficiency     |

## âœ… **Beneficios de esta TransformaciÃ³n**

### **ğŸ¯ Para el MLOps Engineer:**

1. **ğŸ”„ Reproducibilidad**: Mismo resultado siempre
2. **âš¡ AutomatizaciÃ³n**: Integrable en pipelines CI/CD
3. **ğŸ›¡ï¸ Robustez**: Manejo de errores y edge cases
4. **ğŸ“ˆ Escalabilidad**: Procesa cualquier volumen
5. **ğŸ”§ Mantenibilidad**: CÃ³digo modular y testeable
6. **ğŸ¯ Consistencia**: Entrenamiento = ProducciÃ³n

### **ğŸ¯ Para el Modelo ML:**

1. **ğŸ“Š MÃ¡s informaciÃ³n**: 25 features vs 7 originales
2. **ğŸ¯ Mejor predicciÃ³n**: Features especÃ­ficas del dominio
3. **ğŸ”„ Transformaciones aprendidas**: No data leakage
4. **ğŸ“ˆ GeneralizaciÃ³n**: Manejo robusto de datos nuevos

## ğŸ§ª **Testing del Pipeline**

AutomatizaciÃ³n del Script en GitHub Actions

Para llevar todo a un entorno real de integraciÃ³n continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### ğŸ§¾ `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"

jobs:
  data-processing:
    name: Data Processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv

      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/cleaned_house_data.csv

  feature-engineering:
    name: Feature Engineering
    needs: data-processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Engineer features
        run: |
          python src/features/engineer.py --input data/processed/cleaned_house_data.csv --output data/processed/featured_house_data.csv --preprocessor models/trained/preprocessor.pkl
```

> ğŸš€ Esto permite que el procesamiento de datos se ejecute automÃ¡ticamente cada vez que lo dispares manualmente o se cree una nueva versiÃ³n.

---
