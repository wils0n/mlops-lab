# ğŸ§¹ Tutorial MLOps: Data Processing (Limpieza de Datos)

## ğŸ“‹ Tabla de Responsabilidades

| Entregable                   | Responsable                   |
| ---------------------------- | ----------------------------- |
| ğŸ““ Notebooks de exploraciÃ³n  | CientÃ­fico/Ingeniero de datos |
| ğŸ§¹ Scripts de limpieza       | Ingeniero de MLOps            |
| ğŸ”§ Scripts modulares         | Ingeniero de MLOps            |
| ğŸŒ API REST (FastAPI)        | Ingeniero de MLOps            |
| ğŸ“± App prototipo (Streamlit) | Ingeniero de MLOps            |
| ğŸ³ Dockerfile + Compose      | Ingeniero de MLOps            |

# ğŸ“˜ MLOps Workflow: Data Processing desde Notebook a Script

## ğŸŒ Â¿CÃ³mo ve cada rol los datos?

Imagina que formas parte de un equipo multifuncional. Todos trabajan con los mismos datos, pero desde perspectivas distintas:

- ğŸ‘·â€â™‚ï¸ **DevOps Engineer**: piensa en almacenamiento escalable, confiable e infraestructura. Se pregunta: Â¿cÃ³mo aprovisiono esto con Terraform? Â¿cÃ³mo lo monitoreo?
- ğŸ§¹ **Data Engineer (tÃº)**: se enfoca en limpiar, procesar, validar y dejar todo listo para otros roles.
- ğŸ”¬ **Data Scientist**: ve los datos como un tesoro. Quiere analizarlos y entrenar modelos predictivos.

---

## ğŸ§‘â€ğŸ’» Â¿QuÃ© hace un Ingeniero de Datos?

Tu misiÃ³n aquÃ­ es **garantizar la calidad de los datos**. Recuerda: un modelo no puede ser mejor que los datos que recibe.

### ğŸ› ï¸ Responsabilidades clave

- Descubrir y reunir datos de mÃºltiples fuentes (transacciones, logs, APIs, Kaggle).
- Procesarlos para que sean consistentes, completos y Ãºtiles:
  - Detectar y eliminar duplicados.
  - Corregir tipos de datos.
  - Eliminar valores invÃ¡lidos o extremos.
  - Validar reglas lÃ³gicas (ej. que el precio sea mayor a 0).

---

## ğŸ““ Paso 1: ExploraciÃ³n en Notebook

Usamos herramientas como `pandas`, `matplotlib`, `seaborn`, `numpy` para hacer anÃ¡lisis exploratorio:

```python
import pandas as pd
import matplotlib.pyplot as plt

# Carga inicial
df = pd.read_csv('data/raw/house_data.csv')
print(df.shape)
df.head()

# RevisiÃ³n de tipos y nulos
df.info()
df.isna().sum()

# EstadÃ­sticas bÃ¡sicas
df.describe()

# DistribuciÃ³n de precios
df['price'].hist()

# Detectar outliers
plt.boxplot(df['sqft'])
plt.show()
```

> ğŸ” Esta fase te permite conocer el dataset y definir las reglas que aplicarÃ¡s luego en el script.

---

## ğŸ§¼ Paso 2: AutomatizaciÃ³n con Script (`run_processing.py`)

Una vez definido lo que quieres limpiar o corregir, lo transformas en un script ejecutable como este:

```bash
python src/data/run_processing.py   --input data/raw/house_data.csv   --output data/processed/cleaned_house_data.csv
```

### ğŸ§  Ejemplo de contenido del script:

```python
# run_processing.py
import pandas as pd
import numpy as np

# 1. Cargar datos
raw = pd.read_csv(input_file)

# 2. Eliminar duplicados
data = raw.drop_duplicates()

# 3. Eliminar valores extremos
data = data[(data['sqft'] > 300) & (data['price'] > 10000)]

# 4. Eliminar valores negativos
data = data[data.select_dtypes(include=[np.number]) >= 0].dropna()

# 5. Limpiar nombres de columnas
data.columns = data.columns.str.lower().str.replace(' ', '_')

# 6. Guardar datos limpios
data.to_csv(output_file, index=False)
```

> âš™ï¸ Este script automatiza todo lo que descubriste en el anÃ¡lisis exploratorio.

---

## âœ… Resultado del Proceso

- **Entrada:** `data/raw/house_data.csv` con 84 filas.
- **Limpieza aplicada:**
  - Se eliminaron 7 outliers (valores extremos).
  - No habÃ­a valores nulos, pero de existir se imputan o eliminan.
  - Se homogeneizaron los nombres de las columnas.
- **Salida:** `data/processed/cleaned_house_data.csv` con 77 filas.

---

## ğŸ‘©â€ğŸ”¬ Rol del CientÃ­fico de Datos en esta etapa

En esta fase nos ponemos en el lugar de un cientÃ­fico de datos. Su objetivo no es limpiar datos ni desplegar modelos, sino **entender los datos a fondo antes de modelar**.

El cientÃ­fico de datos utiliza herramientas como:

- Jupyter Notebooks o JupyterLab
- Entornos virtuales de Python
- Bibliotecas como `pandas`, `numpy`, `matplotlib`, `seaborn`
- Plataformas como **Databricks** cuando necesita recursos computacionales a escala

No suele preocuparse por contenedores, CI/CD o Git. Solo quiere:

> â€œUn cuaderno, los datos y un kernel que funcioneâ€

---

## ğŸ§ª Â¿QuÃ© es el EDA?

El **AnÃ¡lisis Exploratorio de Datos (EDA)** es una etapa previa al modelado que permite:

- Entender la **estructura estadÃ­stica de los datos**
- Visualizar la **distribuciÃ³n de las variables**
- Identificar correlaciones entre atributos
- Detectar sesgos, desbalance o problemas ocultos

El cientÃ­fico de datos recibe los **datos limpios** del ingeniero de datos y explora:

- Valores mÃ¡ximos y mÃ­nimos
- Medias, medianas, desviaciones
- DistribuciÃ³n de la variable objetivo
- Relaciones entre columnas

---

## ğŸ“˜ Ejemplo aplicado: dataset de precios de casas

### 1. Cargar datos

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('../data/raw/house_data.csv')
df.shape  # filas y columnas
df.head() # primeras observaciones
```

### 2. EstadÃ­sticas y nulos

```python
df.info()
df.describe()
df.isna().sum()
```

### 3. Visualizar distribuciÃ³n del precio

```python
df['price'].hist(bins=30)
```

### 4. CorrelaciÃ³n entre variables

La matriz de correlaciÃ³n (heatmap) nos permite visualizar rÃ¡pidamente quÃ© variables numÃ©ricas estÃ¡n mÃ¡s relacionadas entre sÃ­. Una correlaciÃ³n cercana a 1 indica una relaciÃ³n lineal directa fuerte, mientras que una correlaciÃ³n cercana a -1 indica una relaciÃ³n inversa. Esto es Ãºtil para detectar redundancias (colinealidad) o identificar variables altamente predictivas (como `sqft` y `price`).

```python
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
```

### 5. RelaciÃ³n precio vs pies cuadrados

Este anÃ¡lisis visual muestra cÃ³mo se relaciona la superficie de la casa (en pies cuadrados) con su precio. Es comÃºn encontrar una **relaciÃ³n lineal positiva**, lo que significa que a mayor superficie, mayor suele ser el precio. El diagrama de dispersiÃ³n permite ver esa tendencia general, asÃ­ como posibles valores atÃ­picos o desviaciones que podrÃ­an necesitar revisiÃ³n.

```python
sns.scatterplot(data=df, x="sqft", y="price")
```

### 6. Casas por ubicaciÃ³n (categorÃ­a)

```python
df['location'].value_counts().plot(kind="bar")
```

### 7. Precio promedio por nÃºmero de dormitorios

Este grÃ¡fico muestra cÃ³mo varÃ­a el precio promedio de las viviendas en funciÃ³n del nÃºmero de dormitorios. Es Ãºtil para identificar patrones como: Â¿las casas con mÃ¡s dormitorios cuestan sistemÃ¡ticamente mÃ¡s? Â¿Hay un punto donde el precio se estabiliza o incluso baja? Esta observaciÃ³n puede ayudarte a validar hipÃ³tesis o ajustar variables categÃ³ricas y ordinales.

```python
df.groupby("bedrooms")['price'].mean().plot(kind="bar")
```

---

## ğŸ¯ Â¿Para quÃ© sirve este anÃ¡lisis?

- Detectar **colinealidades** entre variables (ej. sqft y precio)
- Entender **cuÃ¡les atributos son mÃ¡s influyentes**
- Saber si se necesitan nuevas features o si las existentes ya explican bien la salida
- Ver si hay **desbalance** entre categorÃ­as (ubicaciones poco representadas)

---

## ğŸ§  ConclusiÃ³n y siguientes pasos

El EDA te ayuda a formular preguntas como:

- Â¿Puedo predecir el precio solo con `sqft`, `bedrooms` y `location`?
- Â¿Necesito combinar variables (ej. ratio habitaciones/baÃ±os)?
- Â¿Faltan datos para alguna clase o condiciÃ³n?

---

## âš™ï¸ Paso 3: AutomatizaciÃ³n del Script en GitHub Actions

Para llevar todo a un entorno real de integraciÃ³n continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### ğŸ§¾ `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"

jobs:
  data-processing:
    name: Data Processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv
```

> ğŸš€ Esto permite que el procesamiento de datos se ejecute automÃ¡ticamente cada vez que lo dispares manualmente o se cree una nueva versiÃ³n.

---

## ğŸ§­ Siguiente paso

Tu siguiente destino es crear un **pipeline robusto de feature engineering**. IrÃ¡s de un notebook exploratorio a un script modular (`engineer.py`) que puede ejecutarse en producciÃ³n.

ğŸ“„ **Continuar en**: `mlops-workflow-feature-engineering.md`
