# ğŸ§¹ Tutorial MLOps: Data Processing (Limpieza de Datos)

## ğŸ“‹ Tabla de Responsabilidades

| Entregable                   | Responsable                   |
| ---------------------------- | ----------------------------- |
| ğŸ““ Notebooks de exploraciÃ³n  | CientÃ­fico/Ingeniero de datos |
| ğŸ§¹ Scripts de limpieza       | Ingeniero de MLOps            |
| ğŸ”§ Scripts modulares         | Ingeniero de MLOps            |
| ğŸŒ API REST (FastAPI)        | Ingeniero de MLOps            |
| ğŸ“± App prototipo (Streamlit) | Ingeniero de MLOps            |
| ğŸ³ Dockerfile + Compose      | Ingeniero de MLOps            |

# ğŸ“˜ MLOps Workflow: Data Processing desde Notebook a Script

## ğŸŒ Â¿CÃ³mo ve cada rol los datos?

Imagina que formas parte de un equipo multifuncional. Todos trabajan con los mismos datos, pero desde perspectivas distintas:

- ğŸ‘·â€â™‚ï¸ **DevOps Engineer**: piensa en almacenamiento escalable, confiable e infraestructura. Se pregunta: Â¿cÃ³mo aprovisiono esto con Terraform? Â¿cÃ³mo lo monitoreo?
- ğŸ§¹ **Data Engineer (tÃº)**: se enfoca en limpiar, procesar, validar y dejar todo listo para otros roles.
- ğŸ”¬ **Data Scientist**: ve los datos como un tesoro. Quiere analizarlos y entrenar modelos predictivos.

---

## ğŸ§‘â€ğŸ’» Â¿QuÃ© hace un Ingeniero de Datos?

Tu misiÃ³n aquÃ­ es **garantizar la calidad de los datos**. Recuerda: un modelo no puede ser mejor que los datos que recibe.

### ğŸ› ï¸ Responsabilidades clave

- Descubrir y reunir datos de mÃºltiples fuentes (transacciones, logs, APIs, Kaggle).
- Procesarlos para que sean consistentes, completos y Ãºtiles:
  - Detectar y eliminar duplicados.
  - Corregir tipos de datos.
  - Eliminar valores invÃ¡lidos o extremos.
  - Validar reglas lÃ³gicas (ej. que el precio sea mayor a 0).

---

## ğŸ““ Paso 1: ExploraciÃ³n en Notebook

Usamos herramientas como `pandas`, `matplotlib`, `seaborn`, `numpy` para hacer anÃ¡lisis exploratorio:

```python
import pandas as pd
import matplotlib.pyplot as plt

# Carga inicial
df = pd.read_csv('data/raw/house_data.csv')
print(df.shape)
df.head()

# RevisiÃ³n de tipos y nulos
df.info()
df.isna().sum()

# EstadÃ­sticas bÃ¡sicas
df.describe()

# DistribuciÃ³n de precios
df['price'].hist()

# Detectar outliers
plt.boxplot(df['sqft'])
plt.show()
```

> ğŸ” Esta fase te permite conocer el dataset y definir las reglas que aplicarÃ¡s luego en el script.

---

## ğŸ§¼ Paso 2: AutomatizaciÃ³n con Script (`run_processing.py`)

Una vez definido lo que quieres limpiar o corregir, lo transformas en un script ejecutable como este:

```bash
python run_processing.py \
  --input data/raw/house_data.csv \
  --output data/processed/cleaned_house_data.csv
```

### ğŸ§  Ejemplo de contenido del script:

```python
# run_processing.py
import pandas as pd
import numpy as np

# 1. Cargar datos
raw = pd.read_csv(input_file)

# 2. Eliminar duplicados
data = raw.drop_duplicates()

# 3. Eliminar valores extremos
data = data[(data['sqft'] > 300) & (data['price'] > 10000)]

# 4. Eliminar valores negativos
data = data[data.select_dtypes(include=[np.number]) >= 0].dropna()

# 5. Limpiar nombres de columnas
data.columns = data.columns.str.lower().str.replace(' ', '_')

# 6. Guardar datos limpios
data.to_csv(output_file, index=False)
```

> âš™ï¸ Este script automatiza todo lo que descubriste en el anÃ¡lisis exploratorio.

---

## âœ… Resultado del Proceso

- **Entrada:** `data/raw/house_data.csv` con 84 filas.
- **Limpieza aplicada:**
  - Se eliminaron 7 outliers (valores extremos).
  - No habÃ­a valores nulos, pero de existir se imputan o eliminan.
  - Se homogeneizaron los nombres de las columnas.
- **Salida:** `data/processed/cleaned_house_data.csv` con 77 filas.

---

---

## âš™ï¸ Paso 3: AutomatizaciÃ³n del Script en GitHub Actions

Para llevar todo a un entorno real de integraciÃ³n continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### ğŸ§¾ `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"
      run_data_processing:
        description: "Run data processing job"
        required: false
        default: "false"

  release:
    types: [created]
    branches: [main]
    tags: ["v*.*.*"]

jobs:
  data-processing:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv
```

> ğŸš€ Esto permite que el procesamiento de datos se ejecute automÃ¡ticamente cada vez que lo dispares manualmente o se cree una nueva versiÃ³n.

---

## ğŸ§­ Siguiente paso

Tu siguiente destino es crear un **pipeline robusto de feature engineering**. IrÃ¡s de un notebook exploratorio a un script modular (`engineer.py`) que puede ejecutarse en producciÃ³n.

ğŸ“„ **Continuar en**: `mlops-workflow-feature-engineering.md`
